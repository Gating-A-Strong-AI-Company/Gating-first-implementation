{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af5b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98d260",
   "metadata": {},
   "source": [
    "# Each gate is a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082692df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Gate():\n",
    "    \"\"\"\n",
    "    Implements gates for transient rewiring of a network. This is inspired by the paper:\n",
    "    \n",
    "    Nikolić, D. (2023). Where is the mind within the brain? Transient selection of subnetworks \n",
    "    by metabotropic receptors and G protein-gated ion channels. \n",
    "    Computational Biology and Chemistry, 103, 107820.\n",
    "\n",
    "    Note: currently works only for 1D layers of neurons\n",
    "    \n",
    "    x, y: determine which connection is being gated by this gate, where x and y are indexes of neuron in\n",
    "        input and output layer for that weight, respectively.\n",
    "        x can have the value \"bias\" in case the gate gates the bias parameter.\n",
    "        \n",
    "    trigger_layer: in which layer the trigger for this gate is located, input or output. The values\n",
    "            are \"down\" for input and \"up\" for output.\n",
    "            \n",
    "    trigger_neuron_id: the neuron index that will provide inputs for triggering this gate\n",
    "    \n",
    "    g_activated: the gating value g that will be set once the gate is activated. \n",
    "    \n",
    "    activation_threshold: The minimal output value of the trigger neuron necessary to activate the gates. \n",
    "    The default value is 0.1. \n",
    "    \n",
    "    g_default: The value of the gating parameter in its inactivatged state. The deafult value is 1.0.\n",
    "    \n",
    "    duration: deterimines the number of iteratations after which the gate will return back \n",
    "        to its default state. The default number of iterations is 5.\n",
    "    \n",
    "    Usage:\n",
    "    gate = Gate(g, neuron_id)\n",
    "    \n",
    "    \n",
    "    We have the following methods:\n",
    "    \n",
    "    \n",
    "    get_trigger_neuron(): returns the trigger neuron index and the layer (up or down)\n",
    "    \n",
    "    sniff(): check the input and decides whether to activate the gate; it does nothing if the gates is\n",
    "            already active.\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y, trigger_layer, trigger_neuron_id, g_activated, activation_threshold = 0.1, g_default = 1.0, duration = 5):\n",
    "\n",
    "        if x == \"bias\":\n",
    "            self.n_x = -1\n",
    "        else:\n",
    "            self.n_x = x\n",
    "        self.n_y = y\n",
    "        if trigger_layer == \"down\":\n",
    "            self.layer = trigger_layer #up or down\n",
    "        else:\n",
    "            self.layer = \"up\"\n",
    "        self.g_activated = g_activated\n",
    "        self.neuron_id = trigger_neuron_id\n",
    "        self.activation_threshold = activation_threshold\n",
    "        self.g_default = g_default\n",
    "        self.duration = duration\n",
    "        self.state = self.g_default\n",
    "        self.counter = 0\n",
    "        self.activated = False\n",
    "\n",
    "    def get_trigger_neuron(self):\n",
    "        return self.layer, self.neuron_id\n",
    "\n",
    "    def sniff(self, input):\n",
    "        if self.counter == 0 and input > self.activation_threshold:\n",
    "                self.state = self.g_activated\n",
    "                self.counter = self.duration\n",
    "                self.activated = True\n",
    "        self.counter_()\n",
    "\n",
    "    def counter_(self):\n",
    "        if self.counter > 0:\n",
    "            self.counter -= 1\n",
    "        else:\n",
    "            self.state = self.g_default\n",
    "            self.activated =  False\n",
    "            \n",
    "    def de_activate(self):\n",
    "        self.activated = False\n",
    "        self.counter = 0\n",
    "        self.state = self.g_default\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2823d",
   "metadata": {},
   "source": [
    "# Here we expand the functionality of a PyTorch layer to accommodate gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adebb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GatedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    This overwrites the Linear function of the nn module. The new forward function also applies gates\n",
    "    \n",
    "    The constructor receivies a list of gates to operate within this layer\n",
    "    \n",
    "    forward() is extended so that it respects the gate values\n",
    "    \n",
    "    think(): this is a function that updates gates of that layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, set_gates=None, bias=True):\n",
    "        super(GatedLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        if set_gates is not None:\n",
    "            self.gating = True\n",
    "            self.g_weight = nn.Parameter(torch.ones(out_features, in_features))\n",
    "            self.g_weight.requires_grad=False\n",
    "            if bias:\n",
    "                self.g_bias = nn.Parameter(torch.ones(out_features))\n",
    "                self.g_bias.requires_grad=False\n",
    "            else:\n",
    "                self.register_parameter('g_bias', None)\n",
    "            \n",
    "        else:\n",
    "            self.gating = False\n",
    "            self.register_parameter('g_weight', None)\n",
    "            self.register_parameter('g_bias', None)\n",
    "            \n",
    "        self.reset_parameters()        \n",
    "        if set_gates is not None:        \n",
    "            self.gates = set_gates()      \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input, use_gates = True):            \n",
    "        if self.gating is True and use_gates is True:\n",
    "            linear_output = torch.matmul(input, self.weight.t() * self.g_weight.t()) \n",
    "        else:\n",
    "            linear_output = torch.matmul(input, self.weight.t())\n",
    "            \n",
    "        if self.bias is not None:\n",
    "            if self.gating is True and use_gates is True:\n",
    "                linear_output += self.g_bias * self.bias\n",
    "            else:\n",
    "                linear_output += self.bias\n",
    "            \n",
    "        return linear_output\n",
    "\n",
    "    def think(self, output_down, output_up):\n",
    "        self.g_weight.fill_(1.0)\n",
    "        self.g_bias.fill_(1.0)\n",
    "        \n",
    "        for g in self.gates:\n",
    "            layer, n_id = g.get_trigger_neuron()\n",
    "            if layer == \"down\":\n",
    "                g.sniff(output_down[n_id])\n",
    "            else:\n",
    "                g.sniff(output_up[n_id])\n",
    "            \n",
    "            if g.n_x >= 0:\n",
    "                self.g_weight[g.n_x, g.n_y] *= g.state\n",
    "            else:\n",
    "                self.g_bias[g.n_y] *= g.state\n",
    "        return self.gates\n",
    "    \n",
    "    def de_activate_all_gates(self):\n",
    "        for g in self.gates:\n",
    "            g.de_activate()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27287731",
   "metadata": {},
   "source": [
    "# Creating a neural network layer with gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8837989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define gates with different activation behaviors\n",
    "def example_gates():\n",
    "    g1 = Gate(0, 1, \"down\", trigger_neuron_id=0, g_activated=10, activation_threshold=0.5)\n",
    "    g2 = Gate(1, 0, \"down\", trigger_neuron_id=1, g_activated=5, activation_threshold=0.3)\n",
    "    g3 = Gate('bias', 1, \"down\", trigger_neuron_id=0, g_activated=0.2, activation_threshold=0.6)\n",
    "    return [g1, g2, g3]\n",
    "\n",
    "# Instantiate the GatedLinear layer with example gates\n",
    "gl = GatedLinear(2, 2, set_gates=example_gates)\n",
    "\n",
    "# Manually set the weights and biases to observe the effect of gating\n",
    "gl.weight = nn.Parameter(torch.tensor([[0.8, 0.2],\n",
    "                                       [0.3, 0.7]]))\n",
    "gl.bias = nn.Parameter(torch.tensor([0.1, -0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ffd2d",
   "metadata": {},
   "source": [
    "# Define a sequence of inputs to test gating effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78fbf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    torch.tensor([0.2, 0.2]),  # Below threshold, should not activate gates\n",
    "    torch.tensor([0.6, 0.2]),  # Activates g1\n",
    "    torch.tensor([0.3, 0.4]),  # Activates g2\n",
    "    torch.tensor([0.7, 0.6])   # Activates multiple gates\n",
    "]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827be14",
   "metadata": {},
   "source": [
    "# Compare to the outputs without using gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5878d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Outputs with Gates Enabled ===\n",
      "Input 1: [0.2 0.2], Output: [0.3 0. ], Gate States: [1.0, 1.0, 1.0]\n",
      "Input 2: [0.6 0.2], Output: [0.62000006 0.11999999], Gate States: [10, 1.0, 1.0]\n",
      "Input 3: [0.3 0.4], Output: [1.14 0.17], Gate States: [10, 5, 1.0]\n",
      "Input 4: [0.7 0.6], Output: [1.86      1.2699999], Gate States: [10, 5, 0.2]\n",
      "\n",
      "=== Outputs with Gates Disabled ===\n",
      "Input 1: [0.2 0.2], Output: [0.3 0. ]\n",
      "Input 2: [0.6 0.2], Output: [0.62000006 0.11999999]\n",
      "Input 3: [0.3 0.4], Output: [0.42000002 0.17      ]\n",
      "Input 4: [0.7 0.6], Output: [0.78000003 0.43      ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== Outputs with Gates Enabled ===\")\n",
    "for i, inp in enumerate(inputs, 1):\n",
    "    out = gl.forward(inp, use_gates=True)\n",
    "    gl.think(inp, out)  # Update gates based on input and output\n",
    "    print(f\"Input {i}: {inp.numpy()}, Output: {out.detach().numpy()}, Gate States: {[g.state for g in gl.gates]}\")\n",
    "\n",
    "# Reset gate states for comparison\n",
    "gl.de_activate_all_gates()\n",
    "\n",
    "print(\"\\n=== Outputs with Gates Disabled ===\")\n",
    "for i, inp in enumerate(inputs, 1):\n",
    "    out = gl.forward(inp, use_gates=False)\n",
    "    print(f\"Input {i}: {inp.numpy()}, Output: {out.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Outputs\n",
    "\n",
    "In this example, we see how dynamic gating influences the network's output. Each input triggers different gate states, which alter the network's behavior by amplifying or reducing specific connections.\n",
    "\n",
    "#### Outputs with Gates Enabled\n",
    "With gates enabled, the outputs vary based on each input's ability to trigger specific gates. Here’s a breakdown:\n",
    "\n",
    "- **Input 1**: `[0.2, 0.2]`\n",
    "  - **Gate States**: `[1.0, 1.0, 1.0]`\n",
    "    - All gates remain in their default states (1.0), as the inputs are below the activation thresholds.\n",
    "  - **Output**: `[0.3, 0.0]`\n",
    "    - Since the gates haven’t activated, the output is based on the initial weight and bias values without amplification.\n",
    "\n",
    "- **Input 2**: `[0.6, 0.2]`\n",
    "  - **Gate States**: `[10, 1.0, 1.0]`\n",
    "    - `g1` activates because the input `[0.6]` exceeds its threshold of `0.5`, increasing its gating factor to `10`.\n",
    "  - **Output**: `[0.62000006, 0.11999999]`\n",
    "    - The amplified gate factor for `g1` increases the contribution of `Input1` to `Hidden2`, leading to a higher output.\n",
    "\n",
    "- **Input 3**: `[0.3, 0.4]`\n",
    "  - **Gate States**: `[10, 5, 1.0]`\n",
    "    - `g2` activates (since `0.4` exceeds its threshold of `0.3`), while `g1` remains active from the previous input.\n",
    "  - **Output**: `[1.14, 0.17]`\n",
    "    - Both `g1` and `g2` are amplifying their respective connections, further increasing the output.\n",
    "\n",
    "- **Input 4**: `[0.7, 0.6]`\n",
    "  - **Gate States**: `[10, 5, 0.2]`\n",
    "    - `g3` activates because `0.7` exceeds its threshold, reducing its gating factor to `0.2`, while `g1` and `g2` remain active.\n",
    "  - **Output**: `[1.86, 1.2699999]`\n",
    "    - With `g1` and `g2` amplifying their connections and `g3` reducing the bias’s effect, we see a unique output.\n",
    "\n",
    "#### Outputs with Gates Disabled\n",
    "In this section, the outputs are calculated without any gate effects (`use_gates=False`). This comparison highlights the impact of gating:\n",
    "\n",
    "- **Input 1 to Input 4**:\n",
    "  - Outputs are notably lower because the gate factors are set to their defaults (`1.0`), meaning no amplification or reduction is applied.\n",
    "  - This contrast shows how gating dynamically controls connection strengths, enabling unique responses based on the input pattern.\n",
    "\n",
    "### Summary\n",
    "This example demonstrates how gating can selectively amplify or reduce specific connections in the network. Activated gates dynamically adjust outputs by modifying the contribution of individual inputs, allowing the network to adapt its response based on input conditions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
