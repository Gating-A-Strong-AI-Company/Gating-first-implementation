{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af5b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98d260",
   "metadata": {},
   "source": [
    "# Each gate is a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082692df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Gate():\n",
    "    \"\"\"\n",
    "    Implements gates for transient rewiring of a network. This is inspired by the paper:\n",
    "    \n",
    "    Nikolić, D. (2023). Where is the mind within the brain? Transient selection of subnetworks \n",
    "    by metabotropic receptors and G protein-gated ion channels. \n",
    "    Computational Biology and Chemistry, 103, 107820.\n",
    "\n",
    "    Note: currently works only for 1D layers of neurons\n",
    "    \n",
    "    x, y: determine which connection is being gated by this gate, where x and y are indexes of neuron in\n",
    "        input and output layer for that weight, respectively.\n",
    "        x can have the value \"bias\" in case the gate gates the bias parameter.\n",
    "        \n",
    "    trigger_layer: in which layer the trigger for this gate is located, input or output. The values\n",
    "            are \"down\" for input and \"up\" for output.\n",
    "            \n",
    "    trigger_neuron_id: the neuron index that will provide inputs for triggering this gate\n",
    "    \n",
    "    g_activated: the gating value g that will be set once the gate is activated. \n",
    "    \n",
    "    activation_threshold: The minimal output value of the trigger neuron necessary to activate the gates. \n",
    "    The default value is 0.1. \n",
    "    \n",
    "    g_default: The value of the gating parameter in its inactivatged state. The deafult value is 1.0.\n",
    "    \n",
    "    duration: deterimines the number of iteratations after which the gate will return back \n",
    "        to its default state. The default number of iterations is 5.\n",
    "    \n",
    "    Usage:\n",
    "    gate = Gate(g, neuron_id)\n",
    "    \n",
    "    \n",
    "    We have the following methods:\n",
    "    \n",
    "    \n",
    "    get_trigger_neuron(): returns the trigger neuron index and the layer (up or down)\n",
    "    \n",
    "    sniff(): check the input and decides whether to activate the gate; it does nothing if the gates is\n",
    "            already active.\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y, trigger_layer, trigger_neuron_id, g_activated, activation_threshold = 0.1, g_default = 1.0, duration = 5):\n",
    "\n",
    "        if x == \"bias\":\n",
    "            self.n_x = -1\n",
    "        else:\n",
    "            self.n_x = x\n",
    "        self.n_y = y\n",
    "        if trigger_layer == \"down\":\n",
    "            self.layer = trigger_layer #up or down\n",
    "        else:\n",
    "            self.layer = \"up\"\n",
    "        self.g_activated = g_activated\n",
    "        self.neuron_id = trigger_neuron_id\n",
    "        self.activation_threshold = activation_threshold\n",
    "        self.g_default = g_default\n",
    "        self.duration = duration\n",
    "        self.state = self.g_default\n",
    "        self.counter = 0\n",
    "        self.activated = False\n",
    "\n",
    "    def get_trigger_neuron(self):\n",
    "        return self.layer, self.neuron_id\n",
    "\n",
    "    def sniff(self, input):\n",
    "        if self.counter == 0 and input > self.activation_threshold:\n",
    "                self.state = self.g_activated\n",
    "                self.counter = self.duration\n",
    "                self.activated = True\n",
    "        self.counter_()\n",
    "\n",
    "    def counter_(self):\n",
    "        if self.counter > 0:\n",
    "            self.counter -= 1\n",
    "        else:\n",
    "            self.state = self.g_default\n",
    "            self.activated =  False\n",
    "            \n",
    "    def de_activate(self):\n",
    "        self.activated = False\n",
    "        self.counter = 0\n",
    "        self.state = self.g_default\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2823d",
   "metadata": {},
   "source": [
    "# Here we expand the functionality of a PyTorch layer to accommodate gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adebb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GatedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    This overwrites the Linear function of the nn module. The new forward function also applies gates\n",
    "    \n",
    "    The constructor receivies a list of gates to operate within this layer\n",
    "    \n",
    "    forward() is extended so that it respects the gate values\n",
    "    \n",
    "    think(): this is a function that updates gates of that layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, set_gates=None, bias=True):\n",
    "        super(GatedLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        if set_gates is not None:\n",
    "            self.gating = True\n",
    "            self.g_weight = nn.Parameter(torch.ones(out_features, in_features))\n",
    "            self.g_weight.requires_grad=False\n",
    "            if bias:\n",
    "                self.g_bias = nn.Parameter(torch.ones(out_features))\n",
    "                self.g_bias.requires_grad=False\n",
    "            else:\n",
    "                self.register_parameter('g_bias', None)\n",
    "            \n",
    "        else:\n",
    "            self.gating = False\n",
    "            self.register_parameter('g_weight', None)\n",
    "            self.register_parameter('g_bias', None)\n",
    "            \n",
    "        self.reset_parameters()        \n",
    "        if set_gates is not None:        \n",
    "            self.gates = set_gates()      \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input, use_gates = True):            \n",
    "        if self.gating is True and use_gates is True:\n",
    "            linear_output = torch.matmul(input, self.weight.t() * self.g_weight.t()) \n",
    "        else:\n",
    "            linear_output = torch.matmul(input, self.weight.t())\n",
    "            \n",
    "        if self.bias is not None:\n",
    "            if self.gating is True and use_gates is True:\n",
    "                linear_output += self.g_bias * self.bias\n",
    "            else:\n",
    "                linear_output += self.bias\n",
    "            \n",
    "        return linear_output\n",
    "\n",
    "    def think(self, output_down, output_up):\n",
    "        self.g_weight.fill_(1.0)\n",
    "        self.g_bias.fill_(1.0)\n",
    "        \n",
    "        for g in self.gates:\n",
    "            layer, n_id = g.get_trigger_neuron()\n",
    "            if layer == \"down\":\n",
    "                g.sniff(output_down[n_id])\n",
    "            else:\n",
    "                g.sniff(output_up[n_id])\n",
    "            \n",
    "            if g.n_x >= 0:\n",
    "                self.g_weight[g.n_x, g.n_y] *= g.state\n",
    "            else:\n",
    "                self.g_bias[g.n_y] *= g.state\n",
    "        return self.gates\n",
    "    \n",
    "    def de_activate_all_gates(self):\n",
    "        for g in self.gates:\n",
    "            g.de_activate()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27287731",
   "metadata": {},
   "source": [
    "# Creating a neural network layer with gates in 2 different regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8837989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define gates for region 1\n",
    "def region1_gates():\n",
    "    g1 = Gate(0, 1, \"down\", trigger_neuron_id=0, g_activated=10, activation_threshold=0.5)\n",
    "    g2 = Gate(1, 0, \"down\", trigger_neuron_id=1, g_activated=5, activation_threshold=0.3)\n",
    "    return [g1, g2]\n",
    "\n",
    "# Define gates for region 2\n",
    "def region2_gates():\n",
    "    g3 = Gate(0, 1, \"down\", trigger_neuron_id=0, g_activated=0.1, activation_threshold=0.4)\n",
    "    g4 = Gate(1, 0, \"down\", trigger_neuron_id=1, g_activated=2, activation_threshold=0.6)\n",
    "    return [g3, g4]\n",
    "\n",
    "# Instantiate two GatedLinear layers, each with a different set of gates\n",
    "gl_region1 = GatedLinear(2, 2, set_gates=region1_gates)\n",
    "gl_region2 = GatedLinear(2, 2, set_gates=region2_gates)\n",
    "\n",
    "# Manually set the weights and biases to observe the effect of gating\n",
    "gl_region1.weight = nn.Parameter(torch.tensor([[0.8, 0.2],\n",
    "                                               [0.3, 0.7]]))\n",
    "gl_region1.bias = nn.Parameter(torch.tensor([0.1, -0.2]))\n",
    "\n",
    "gl_region2.weight = nn.Parameter(torch.tensor([[0.5, 0.5],\n",
    "                                               [0.4, 0.6]]))\n",
    "gl_region2.bias = nn.Parameter(torch.tensor([0.2, -0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ffd2d",
   "metadata": {},
   "source": [
    "# Define a sequence of inputs to test gating effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78fbf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequence of inputs to test alternating gating effects\n",
    "inputs = [\n",
    "    torch.tensor([0.2, 0.2]),  # Should only activate low-threshold gates\n",
    "    torch.tensor([0.6, 0.2]),  # Will activate region1 more\n",
    "    torch.tensor([0.3, 0.4]),  # Mixed activation\n",
    "    torch.tensor([0.7, 0.6])   # Strong activation in region2\n",
    "]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827be14",
   "metadata": {},
   "source": [
    "# Compare to the outputs without using gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5878d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Alternating Outputs for Each Region ===\n",
      "Input 1 (Region 1): [0.2 0.2], Output: [0.3 0. ], Gate States: [1.0, 1.0]\n",
      "Input 2 (Region 2): [0.6 0.2], Output: [0.6        0.26000002], Gate States: [0.1, 1.0]\n",
      "Input 3 (Region 1): [0.3 0.4], Output: [0.42000002 0.17      ], Gate States: [1.0, 5]\n",
      "Input 4 (Region 2): [0.7 0.6], Output: [0.58 0.54], Gate States: [0.1, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Control variable for alternating between regions\n",
    "toggle_region = True  # Start with region 1\n",
    "\n",
    "print(\"=== Alternating Outputs for Each Region ===\")\n",
    "for i, inp in enumerate(inputs, 1):\n",
    "    if toggle_region:\n",
    "        # Use Region 1 Gates\n",
    "        out = gl_region1.forward(inp, use_gates=True)\n",
    "        gl_region1.think(inp, out)  # Update gates in region 1\n",
    "        print(f\"Input {i} (Region 1): {inp.numpy()}, Output: {out.detach().numpy()}, Gate States: {[g.state for g in gl_region1.gates]}\")\n",
    "    else:\n",
    "        # Use Region 2 Gates\n",
    "        out = gl_region2.forward(inp, use_gates=True)\n",
    "        gl_region2.think(inp, out)  # Update gates in region 2\n",
    "        print(f\"Input {i} (Region 2): {inp.numpy()}, Output: {out.detach().numpy()}, Gate States: {[g.state for g in gl_region2.gates]}\")\n",
    "    \n",
    "    # Toggle region for the next input\n",
    "    toggle_region = not toggle_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Alternating Outputs for Each Region\n",
    "\n",
    "This example demonstrates how the outputs change when toggling between two gate regions. Each region has a distinct set of gates with specific activation thresholds and amplification/reduction factors, resulting in different outputs even for the same input pattern. By alternating between Region 1 and Region 2, we observe repeatable outputs for each region when it is activated.\n",
    "\n",
    "#### Results Breakdown\n",
    "\n",
    "- **Input 1 (Region 1)**: `[0.2, 0.2]`\n",
    "  - **Gate States**: `[1.0, 1.0]`\n",
    "    - The input values are below the activation thresholds for Region 1’s gates (`g1` and `g2`), so both gates remain in their default state with a gating factor of `1.0`.\n",
    "  - **Output**: `[0.3, 0.0]`\n",
    "    - Since no gates are activated, the output is determined purely by the initial weights and biases of Region 1’s `GatedLinear` layer.\n",
    "\n",
    "- **Input 2 (Region 2)**: `[0.6, 0.2]`\n",
    "  - **Gate States**: `[0.1, 1.0]`\n",
    "    - Region 2 is now active, and `g3` activates with a gating factor of `0.1` because `0.6` exceeds its threshold of `0.4`. `g4` remains in the default state of `1.0`.\n",
    "  - **Output**: `[0.6, 0.26000002]`\n",
    "    - The activated gate (`g3`) reduces the impact of the connection it controls, leading to a different output pattern compared to Region 1.\n",
    "\n",
    "- **Input 3 (Region 1)**: `[0.3, 0.4]`\n",
    "  - **Gate States**: `[1.0, 5]`\n",
    "    - Region 1 is reactivated, and `g2` activates with a gating factor of `5` because `0.4` exceeds its threshold of `0.3`. `g1` remains inactive.\n",
    "  - **Output**: `[0.42000002, 0.17]`\n",
    "    - With `g2` amplifying its connection, the output reflects this increased contribution, showing a unique pattern for Region 1 with this input.\n",
    "\n",
    "- **Input 4 (Region 2)**: `[0.7, 0.6]`\n",
    "  - **Gate States**: `[0.1, 1.0]`\n",
    "    - Back to Region 2, `g3` activates with a gating factor of `0.1` (due to its low activation threshold of `0.4`), while `g4` stays inactive at `1.0`.\n",
    "  - **Output**: `[0.58, 0.54]`\n",
    "    - Region 2 produces a consistent output for the same gate configuration as seen with Input 2, demonstrating repeatable behavior for this region's settings.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This output illustrates how different regions of gates, with distinct thresholds and amplification factors, result in unique outputs. By alternating between Region 1 and Region 2, we observe consistent outputs for each region when revisited, demonstrating the controlled, repeatable influence of gating configurations on network behavior.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
