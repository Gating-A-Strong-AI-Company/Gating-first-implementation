{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5b6ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDas Ausführen von Zellen mit \"Python 3.12.7\" erfordert das Paket ipykernel.\n",
      "\u001b[1;31mFühren Sie den folgenden Befehl aus, um „ipykernel“ in der Python-Umgebung zu installieren. \n",
      "\u001b[1;31mBefehl: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98d260",
   "metadata": {},
   "source": [
    "# Each gate is a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082692df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Gate():\n",
    "    \"\"\"\n",
    "    Implements gates for transient rewiring of a network. This is inspired by the paper:\n",
    "    \n",
    "    Nikolić, D. (2023). Where is the mind within the brain? Transient selection of subnetworks \n",
    "    by metabotropic receptors and G protein-gated ion channels. \n",
    "    Computational Biology and Chemistry, 103, 107820.\n",
    "\n",
    "    Note: currently works only for 1D layers of neurons\n",
    "    \n",
    "    x, y: determine which connection is being gated by this gate, where x and y are indexes of neuron in\n",
    "        input and output layer for that weight, respectively.\n",
    "        x can have the value \"bias\" in case the gate gates the bias parameter.\n",
    "        \n",
    "    trigger_layer: in which layer the trigger for this gate is located, input or output. The values\n",
    "            are \"down\" for input and \"up\" for output.\n",
    "            \n",
    "    trigger_neuron_id: the neuron index that will provide inputs for triggering this gate\n",
    "    \n",
    "    g_activated: the gating value g that will be set once the gate is activated. \n",
    "    \n",
    "    activation_threshold: The minimal output value of the trigger neuron necessary to activate the gates. \n",
    "    The default value is 0.1. \n",
    "    \n",
    "    g_default: The value of the gating parameter in its inactivatged state. The deafult value is 1.0.\n",
    "    \n",
    "    duration: deterimines the number of iteratations after which the gate will return back \n",
    "        to its default state. The default number of iterations is 5.\n",
    "    \n",
    "    Usage:\n",
    "    gate = Gate(g, neuron_id)\n",
    "    \n",
    "    \n",
    "    We have the following methods:\n",
    "    \n",
    "    \n",
    "    get_trigger_neuron(): returns the trigger neuron index and the layer (up or down)\n",
    "    \n",
    "    sniff(): check the input and decides whether to activate the gate; it does nothing if the gates is\n",
    "            already active.\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y, trigger_layer, trigger_neuron_id, g_activated, activation_threshold = 0.1, g_default = 1.0, duration = 5):\n",
    "\n",
    "        if x == \"bias\":\n",
    "            self.n_x = -1\n",
    "        else:\n",
    "            self.n_x = x\n",
    "        self.n_y = y\n",
    "        if trigger_layer == \"down\":\n",
    "            self.layer = trigger_layer #up or down\n",
    "        else:\n",
    "            self.layer = \"up\"\n",
    "        self.g_activated = g_activated\n",
    "        self.neuron_id = trigger_neuron_id\n",
    "        self.activation_threshold = activation_threshold\n",
    "        self.g_default = g_default\n",
    "        self.duration = duration\n",
    "        self.state = self.g_default\n",
    "        self.counter = 0\n",
    "        self.activated = False\n",
    "\n",
    "    def get_trigger_neuron(self):\n",
    "        return self.layer, self.neuron_id\n",
    "\n",
    "    def sniff(self, input):\n",
    "        if self.counter == 0 and input > self.activation_threshold:\n",
    "                self.state = self.g_activated\n",
    "                self.counter = self.duration\n",
    "                self.activated = True\n",
    "        self.counter_()\n",
    "\n",
    "    def counter_(self):\n",
    "        if self.counter > 0:\n",
    "            self.counter -= 1\n",
    "        else:\n",
    "            self.state = self.g_default\n",
    "            self.activated =  False\n",
    "            \n",
    "    def de_activate(self):\n",
    "        self.activated = False\n",
    "        self.counter = 0\n",
    "        self.state = self.g_default\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2823d",
   "metadata": {},
   "source": [
    "# Here we expand the functionality of a PyTorch layer to accommodate gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GatedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    This overwrites the Linear function of the nn module. The new forward function also applies gates\n",
    "    \n",
    "    The constructor receivies a list of gates to operate within this layer\n",
    "    \n",
    "    forward() is extended so that it respects the gate values\n",
    "    \n",
    "    think(): this is a function that updates gates of that layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, set_gates=None, bias=True):\n",
    "        super(GatedLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        if set_gates is not None:\n",
    "            self.gating = True\n",
    "            self.g_weight = nn.Parameter(torch.ones(out_features, in_features))\n",
    "            self.g_weight.requires_grad=False\n",
    "            if bias:\n",
    "                self.g_bias = nn.Parameter(torch.ones(out_features))\n",
    "                self.g_bias.requires_grad=False\n",
    "            else:\n",
    "                self.register_parameter('g_bias', None)\n",
    "            \n",
    "        else:\n",
    "            self.gating = False\n",
    "            self.register_parameter('g_weight', None)\n",
    "            self.register_parameter('g_bias', None)\n",
    "            \n",
    "        self.reset_parameters()        \n",
    "        if set_gates is not None:        \n",
    "            self.gates = set_gates()      \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input, use_gates = True):            \n",
    "        if self.gating is True and use_gates is True:\n",
    "            linear_output = torch.matmul(input, self.weight.t() * self.g_weight.t()) \n",
    "        else:\n",
    "            linear_output = torch.matmul(input, self.weight.t())\n",
    "            \n",
    "        if self.bias is not None:\n",
    "            if self.gating is True and use_gates is True:\n",
    "                linear_output += self.g_bias * self.bias\n",
    "            else:\n",
    "                linear_output += self.bias\n",
    "            \n",
    "        return linear_output\n",
    "\n",
    "    def think(self, output_down, output_up):\n",
    "        self.g_weight.fill_(1.0)\n",
    "        self.g_bias.fill_(1.0)\n",
    "        \n",
    "        for g in self.gates:\n",
    "            layer, n_id = g.get_trigger_neuron()\n",
    "            if layer == \"down\":\n",
    "                g.sniff(output_down[n_id])\n",
    "            else:\n",
    "                g.sniff(output_up[n_id])\n",
    "            \n",
    "            if g.n_x >= 0:\n",
    "                self.g_weight[g.n_x, g.n_y] *= g.state\n",
    "            else:\n",
    "                self.g_bias[g.n_y] *= g.state\n",
    "        return self.gates\n",
    "    \n",
    "    def de_activate_all_gates(self):\n",
    "        for g in self.gates:\n",
    "            g.de_activate()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27287731",
   "metadata": {},
   "source": [
    "# Creating a neural network layer with gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_gates():\n",
    "    g1 = Gate(0, 1, \"down\", trigger_neuron_id = 0, g_activated = 10)\n",
    "    g2 = Gate(1, 0, \"down\", trigger_neuron_id = 1, g_activated = 10)\n",
    "    g3 = Gate('bias', 1, \"down\", trigger_neuron_id = 0, g_activated = 0.1)\n",
    "    return [g1, g2, g3]\n",
    "\n",
    "gl = GatedLinear(2,2, set_gates = my_gates)\n",
    "\n",
    "gl.weight = nn.Parameter(torch.tensor([[ 0.5, 0.5],\n",
    "                                        [0.5, 0.5]]))\n",
    "gl.bias = nn.Parameter(torch.tensor([0.0, -.5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ffd2d",
   "metadata": {},
   "source": [
    "# Create a sequence of inputs for the layer to iterate through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inps = [torch.tensor([0.0,0.0]), \n",
    "        torch.tensor([0.0,1.0]), \n",
    "        torch.tensor([1.0,0.0]), \n",
    "        torch.tensor([1.0,1.0])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b393a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for inp in inps:\n",
    "    out = gl.forward(inp, use_gates = True)\n",
    "    gl.think(inp, out)\n",
    "    print(out)\n",
    "gl.de_activate_all_gates()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827be14",
   "metadata": {},
   "source": [
    "# Compare to the outputs without using gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5878d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for inp in inps:\n",
    "    out = gl.forward(inp, use_gates = False)\n",
    "    print(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
